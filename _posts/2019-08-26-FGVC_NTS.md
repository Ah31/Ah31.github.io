---
layout: post
title: Understanding Fine Grained Visual Classification with NTS

---
Computer Vision has excelled enormously since the 2012 ILSVRC Competition won by Alexnet. This is the statement one comes across often when starting with this fast growing technology. This blog is to understand the challenging problem of Fine Grained Visual Classification(FGVC) which will be described in detail using the following paper:- 

[**Learning to Navigate for Fine-grained Classification**](https://arxiv.org/abs/1809.00287)

For the Pytorch code implementation, refer to the following github repository.
(https://github.com/yangze0930/NTS-Net)

In the process, one may understand the challenges that might be faced initially and how to reach 87% accuracy using interesting architecture of this paper from 42% when just starting out (numbers are based on my experience). The dataset used is FGVC Aircraft variant. Later on, I had trained NTS on Stanford-Cars as well.
This article is divided into following sections :-

**Part 1 narrates briefly about my initial journey and challenges faced while starting with this problem.**

**Part 2 describes about the research paper mentioned above where Sections 1,2 and 3 describe about Region Proposal , its usage in NTS and various custom losses respectively.**

**PART 1 :- INITIAL TRIALS AND ERRORS**

**FINE GRAINED VISUAL CLASSIFICATION**

We know that visual classification task means creating a model that captures relationship from input images to their corresponding output classes. However the task of FGVC differs from the norm because of more intra-class variance over inter-class. That is why our aim is to capture discriminative features among visually similar classes. Finding such features is challenging. Moreover, gathering image samples with bounding box annotations referring to the most informative regions in each sample is expensive.

While starting out with the problem, one might use the general way of performing image classification i.e. use standard pre-trained models and fine tune them to achieve right set of parameters for your task. As specified in the paper for the dataset linked below, there are three levels of hierarchy at which classification is done viz. at the manufacturer, family and variant levels. The classification at the variant level is fine grained.

[**Fine-Grained Visual Classification of Aircraft**](https://arxiv.org/abs/1306.5151)

In my journey to solve FGVC on above mentioned dataset, I had started with the standard pre-trained models and tried different tweaks that could give 66% validation accuracy on manufacturer and 42% on variant level before attempting NTS which provided 87% on varaint level in the first go and it also came with a plus point. For a beginner in the field like me, it was really helpful to understand the working code of a complex problem and get proper insights.

----


**PART 2 :- WHAT WORKED**

**NTS-NET**

As we discussed before, gathering samples with bounding box annotations that refer to the most informative regions in each sample, is expensive. This is where this paper helps in an outstanding manner. It effectively localizes those informative regions without needing bounding box annotations.
The model created in this paper, termed as NTS-NET uses three “agents” working cooperatively to achieve state-of-the-art performance in benchmark datasets viz. FGVC aircraft, Stanford Cars, Caltech-UCSD Birds.
The three agents are defined as Navigator, Teacher and Scrutinizer. Let us discuss on the roles of these.

**The Navigator agent/network** navigates the model to focus on most informative regions. For each region in the image, navigator predicts how much informative the region is through ranking loss(described below), and the predictions are used to propose the most informative regions. Now the question comes : How to get helpful variable length ‘regions’ in the image? Well this is answered ahead so bear with me for a while to understand the high-level functionality of each of these agents.

**Teacher agent** evaluates the most informative regions proposed by Navigator and provides feedback :- for each proposed region, the teacher evaluates it’s probability being ground-truth class. Confidence evaluations guide the navigator network to propose more informative regions using the ordering-consistent loss function which is termed ‘ranking loss’ in the code implementation.

> With more precise supervision provided by teacher, navigator will localize more informative regions, which in turn will benefit the  teacher.

**Scrutinizer agent** scrutinizes the proposed regions from Navigator and makes fine-grained classification: each proposed region is enlarged to same size and the agent extracts features therein ; the features of regions and of the whole image are jointly processed to make fine-grained classification which is the main approach working as an outstanding solution to this complex problem.

**Informative regions are helpful to better characterize the object, so fusing features from the informative regions and the full image will achieve better performance.
Hence goal is to localize the most informative regions of the objects.**

![_config.yml]({{ site.baseurl }}/images/FGVC-1.png "NTS Model Architecture")
                    -- **_Fig 1:- NTS Model Architecture_**

Now let us come to the question we discussed above viz. How to get helpful variable length ‘regions’ in the image? Navigating to possible informative regions can be viewed as a Region Proposal Network (RPN) problem which was introduced in R-CNN paper and I will discuss it’s relevance here.

**SECTION 1:- REGION PROPOSAL**

Before discussing how Region Proposal is done in NTS, I should give a short intro about its origins. Feel free to skip this section if you know about the topic.
So there have been few ways introduced to propose regions as follows:-
i) Sliding Windows:- In sliding windows, you run a training classifier over all the windows of same fixed size slided over the image and then run detector to see what is the object. We could use this algorithm but the downside is that it may go through many such windows having no objects to classify. Hence the algorithm R-CNN was proposed.

ii) R-CNN:- Here segmentation algorithm is run to obtain regions which may contain objects and run the classifier only on those regions. The downside here was slowness because the proposed regions were classified one at a time.

iii) Fast R-CNN:- Segmentation algorithm is used to propose regions but in contrast to R-CNN all the proposed regions were classified at the same time using convolutional implementation of sliding windows. Please refer to the remarkable video of Andrew Ng for detailed understanding on this. Link is provided below:
<https://www.coursera.org/lecture/convolutional-neural-networks/convolutional-implementation-of-sliding-windows-6UnU4>

iv) Faster R-CNN:- Uses Region Proposal Network abbreviated as RPN which needs both anchors( bounding boxes that are placed throughout the image with different sizes,scales and aspect ratios) and ground truth bounding boxes to propose informative regions instead of traditional segmentation algorithm. To better understand Faster R-CNN, please refer [here](https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8).

**SECTION 2: HOW REGIONS ARE PROPOSED IN NTS?**

In this paper, default anchors are placed throughout the whole image and the NTS-model learns most informative anchors out of these through a custom loss termed ranking loss in the code implementation(learns because we are not using annotated bounding boxes). These anchors define the coordinates of proposed regions given by Prosposal_Net(or Navigator Network) as defined in the code and using NMS(Non-maximum suppression) we remove region redundancy(overlapping regions) and give the top_n proposed regions.

![_config.yml]({{ site.baseurl }}/images/FGVC-Aircraft.png "TOP 3 most informative regions learned by NTS (dataset used for training:-FGVC Aircraft)")
                   -- **_Fig 2:- TOP 3 most informative regions learned by NTS (dataset used for training:-FGVC Aircraft)_**


![_config.yml]({{ site.baseurl }}/images/FGVC-2.png "TOP 2 most informative regions learned by NTS (dataset used for training:-Stanford-Cars)")
                    -- **_Fig 3:- TOP 2 most informative regions learned by NTS (dataset used for training:-Stanford-Cars)_**



**SECTION 3:- DESCRIPTION OF VARIOUS LOSS USED IN NTS-NET**

There are many custom loss used in the paper code later to be accumulated in total loss namely, raw loss, concat loss, rank loss and the part_cls loss.

**NOTE**: The terms for losses used in the code are different from those presented in the paper viz. Navigator, Teacher and Scrutinizer loss and here i am using the ones used in the code. Please try to understand the code for practically knowing what is happening in the model.

**Total loss** = Raw_loss+Rank_loss+Concat_loss+Part_cls_loss

**NOTE**:-We use RESNET50 model as feature extractor of original image and proposed regions.

* **RAW LOSS** :- This is the categorical cross entropy loss for image classification that targets RESNET network parameters. We perform raw loss on original image features which will later be combined with that of proposed region image for fine grained classification. Here the output is image’s label.

* **CONCAT LOSS** :- In Scrutinizer network, we concat features from both the original image and the proposed regions which are input to this categorical cross entropy loss and the output is the image’s label.

* **PART LOSS(LIST LOSS)** :- It is used as feedback to navigator network as here we find the cross entropy loss between every proposed image and its ground-truth class.

* **RANK LOSS**:- Using the top_n rpn scores(proposed region features) and the corresponding loss for every proposed region received as feedback from PART LOSS, following is done:- for every proposal, all the losses higher than that of proposal are added in the rank loss such that they can be optimized.

* **PART_CLS LOSS**:- This is the cross entropy loss between part features and labels. The part features are extracted from RESNET-50 using part_images as defined in code which are generated from the original image using the top_n region proposals’ coordinates.

Now part loss and part_cls loss are same but part_cls loss contributes to the overall(total) loss while the other does not and is used in rank loss as guide/feedback.

----

I tried to explain the paper such that those who are interested to understand the code may also find this post useful and therefore mainly used the terminologies of the code.

Thank you for reading! :)

References:

* [Learning to Navigate for Fine-grained Classiﬁcation](https://arxiv.org/pdf/1809.00287.pdf)
* (https://www.coursera.org/lecture/convolutional-neural-networks/convolutional-implementation-of-sliding-windows-6UnU4)
* [Fine-Grained Visual Classification of Aircraft](https://arxiv.org/abs/1306.5151)
* (https://github.com/yangze0930/NTS-Net)
* Andrew Ng CNN Coursera (Region proposal origins)

